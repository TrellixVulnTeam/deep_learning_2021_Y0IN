{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd03a227a81d8a280ae5e926c470af71c1cc02f5f6bd9a08b8bc0ce614506c13b39",
   "display_name": "Python 3.7.10 64-bit ('tf': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_valid_data(valid_ratio=0.2, show_info=False):\n",
    "    import mnist\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    images, labels = mnist.train_images(), mnist.train_labels()\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "            images, labels, test_size=valid_ratio, random_state=111)\n",
    "\n",
    "    if show_info:\n",
    "        print(\"Train images:\", type(x_train), x_train.shape, x_train.dtype)\n",
    "        print(\"Train labels:\", type(y_train), y_train.shape, y_train.dtype)\n",
    "        print(\"Valid images:\", type(x_valid), x_valid.shape, x_valid.dtype)\n",
    "        print(\"Valid labels:\", type(y_valid), y_valid.shape, y_valid.dtype)\n",
    "\n",
    "    return (x_train, y_train), (x_valid, y_valid)\n",
    "\n",
    "\n",
    "def load_test_data():\n",
    "    import mnist\n",
    "\n",
    "    return mnist.test_images(), mnist.test_labels()\n",
    "\n",
    "\n",
    "def preprocess(x, y):\n",
    "    x = x.reshape(-1, 784).astype(\"float32\")/255.\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train images: <class 'numpy.ndarray'> (48000, 784) float32\nTrain labels: <class 'numpy.ndarray'> (48000,) uint8\nValid images: <class 'numpy.ndarray'> (12000, 784) float32\nValid labels: <class 'numpy.ndarray'> (12000,) uint8\n"
     ]
    }
   ],
   "source": [
    "## Hyper parameters\n",
    "n_epochs = 10\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "## Load dataset\n",
    "(x_train, y_train), (x_valid, y_valid) = load_train_valid_data()\n",
    "x_train, y_train = preprocess(x_train, y_train)\n",
    "x_valid, y_valid = preprocess(x_valid, y_valid)\n",
    "\n",
    "print(\"Train images:\", type(x_train), x_train.shape, x_train.dtype)\n",
    "print(\"Train labels:\", type(y_train), y_train.shape, y_train.dtype)\n",
    "print(\"Valid images:\", type(x_valid), x_valid.shape, x_valid.dtype)\n",
    "print(\"Valid labels:\", type(y_valid), y_valid.shape, y_valid.dtype)\n",
    "\n",
    "\n",
    "## Model / Loss function / Optimizer\n",
    "input_size = x_train.shape[0]\n",
    "output_size = int(y_train.max()) + 1\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(200, activation='relu'),\n",
    "    tf.keras.layers.Dense(200, activation='relu'),\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax'),\n",
    "])\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optim = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "48000/48000 [==============================] - 2s 45us/sample - loss: 0.3906 - val_loss: 0.1940\n",
      "Epoch 2/10\n",
      "48000/48000 [==============================] - 2s 32us/sample - loss: 0.1507 - val_loss: 0.1327\n",
      "Epoch 3/10\n",
      "48000/48000 [==============================] - 2s 32us/sample - loss: 0.1026 - val_loss: 0.1094\n",
      "Epoch 4/10\n",
      "48000/48000 [==============================] - 1s 31us/sample - loss: 0.0772 - val_loss: 0.1035\n",
      "Epoch 5/10\n",
      "48000/48000 [==============================] - 1s 31us/sample - loss: 0.0590 - val_loss: 0.0872\n",
      "Epoch 6/10\n",
      "48000/48000 [==============================] - 2s 31us/sample - loss: 0.0434 - val_loss: 0.0897\n",
      "Epoch 7/10\n",
      "48000/48000 [==============================] - 2s 32us/sample - loss: 0.0347 - val_loss: 0.0843\n",
      "Epoch 8/10\n",
      "48000/48000 [==============================] - 2s 32us/sample - loss: 0.0259 - val_loss: 0.0902\n",
      "Epoch 9/10\n",
      "48000/48000 [==============================] - 2s 32us/sample - loss: 0.0201 - val_loss: 0.0789\n",
      "Epoch 10/10\n",
      "48000/48000 [==============================] - 2s 31us/sample - loss: 0.0168 - val_loss: 0.0920\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa5e04da450>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "model.compile(loss=loss_fn, optimizer=optim)\n",
    "model.fit(x_train, y_train, validation_data=(x_valid, y_valid),\n",
    "    epochs=n_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "[Training with validation]\n",
      "Epoch [  1/ 10] >>> train_loss = 1.44e-02, valid_loss = 8.55e-02\n",
      "Epoch [  2/ 10] >>> train_loss = 1.03e-02, valid_loss = 8.44e-02\n",
      "Epoch [  3/ 10] >>> train_loss = 8.21e-03, valid_loss = 8.57e-02\n",
      "Epoch [  4/ 10] >>> train_loss = 8.50e-03, valid_loss = 8.85e-02\n",
      "Epoch [  5/ 10] >>> train_loss = 4.77e-03, valid_loss = 9.23e-02\n",
      "Epoch [  6/ 10] >>> train_loss = 5.06e-03, valid_loss = 1.01e-01\n",
      "Epoch [  7/ 10] >>> train_loss = 4.39e-03, valid_loss = 9.20e-02\n",
      "Epoch [  8/ 10] >>> train_loss = 2.84e-03, valid_loss = 9.10e-02\n",
      "Epoch [  9/ 10] >>> train_loss = 1.26e-03, valid_loss = 8.94e-02\n",
      "Epoch [ 10/ 10] >>> train_loss = 6.46e-04, valid_loss = 9.32e-02\n"
     ]
    }
   ],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).shuffle(buffer_size=10000).batch(batch_size)\n",
    "\n",
    "valid_data = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_valid, y_valid)).batch(batch_size)\n",
    "\n",
    "print(\"\\n[Training with validation]\")\n",
    "history = {'train_loss':[], 'valid_loss':[]}\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    ## Training\n",
    "    batch_loss = []\n",
    "    for xi, yi in train_data:\n",
    "        with tf.GradientTape() as tape:\n",
    "            yi_hat = model(xi, training=True)\n",
    "            loss = loss_fn(yi, yi_hat)\n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_weights)\n",
    "        optim.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        batch_loss.append(float(loss))\n",
    "\n",
    "    train_loss = sum(batch_loss)/len(batch_loss)\n",
    "\n",
    "    ## Validation\n",
    "    batch_loss = []\n",
    "    for xi, yi in valid_data:\n",
    "        yi_hat = model(xi, training=False)\n",
    "        loss = loss_fn(yi, yi_hat)\n",
    "        batch_loss.append(float(loss))\n",
    "\n",
    "    valid_loss = sum(batch_loss)/len(batch_loss)\n",
    "\n",
    "    ## Print log\n",
    "    print(\"Epoch [%3d/%3d] >>> train_loss = %.2e, valid_loss = %.2e\" \n",
    "        % (epoch+1, n_epochs, train_loss, valid_loss))\n",
    "\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['valid_loss'].append(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=245676, shape=(4,), dtype=int32, numpy=array([90, 30, 60,  0], dtype=int32)>,\n",
       " <tf.Tensor: id=245680, shape=(4,), dtype=int32, numpy=array([80, 10, 50, 70], dtype=int32)>,\n",
       " <tf.Tensor: id=245684, shape=(2,), dtype=int32, numpy=array([40, 20], dtype=int32)>]"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "batch_size = 4\n",
    "data = tf.range(10)*10\n",
    "\n",
    "data_size = data.shape[0] # 10\n",
    "steps_per_epoch = data_size // batch_size + (1 if data_size % batch_size else 0) # 3\n",
    "\n",
    "indices = tf.random.shuffle(tf.range(data.shape[0]))\n",
    "x = tf.gather(data, indices=indices)\n",
    "\n",
    "[x[i*batch_size:(i+1)*batch_size] for i in range(steps_per_epoch)]\n",
    "\n",
    "\n",
    "# [batch_size for _ in range(data.shape[0] // batch_size)]"
   ]
  }
 ]
}